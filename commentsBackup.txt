1-Historically Swift strings have matched ObjC strings: UTF16, with an ASCII optimisation path.For Swift 5 and as part of the ABI stabilisation effort, they're switching to an UTF8 storage model with some tricks for efficient Cocoa interop (basically caching UTF16 indices for amortised but not proper O(1) access).Interestingly/oddly, the SSO ABI only stores 15 code units in 64b.-0-0-23-0-0.4--1
2-&gt;Interestingly/oddly, the SSO ABI only stores 15 code units in 64b.~~max size of UTF8 code unit in bytes: 4. 4 \* 15 = 60 bytes, leaving 4 bytes of overhead.~~ A code unit is not a code point, this is wrong.~~Overhead is minimally 6 bits for the size of the small string, leaving 26 bits for the rest of the type. I know literally nothing about Swift's particular runtime, but even if those 26 bits are used form a vtable pointer to the StringSSO implementation, that's not a *huge* waste... right?~~I think the interesting choices here are:1. short string optimization (SSO) is important enough to bake into an ABI. Even though you marry yourself to a definition of "short".2. In 2018, Apple's definition of "short string" includes *enduser text* where endusers are from all over the globe. So they picked 64 bytes, which is pretty big.If you want your SSO string to be 32 bytes, you can only fit 3 emoji flags. And I think like a billion people only write languages where each character is 3 bytes in UTF8. Apple believes the code point distribution that Swift applications will see will skew much higher than most compilers have historically anticipated.*\** The 5byte and 6byte encodings for UTF8 are deprecated by [RFC 3629](https://tools.ietf.org/html/rfc3629)-0-0-6-0-0.6-1
3-&gt; max size of UTF8 code unit in bytes: 4No, the code unit of utf8 is 8 bits. There is no min/max size, it’s always 8 bits. So your entire computation is wrong, they’re storing 15 bytes (16 really if you account for niltermination) — in the same way they stored 15 ascii code units (7 bits) — which can represent 4 to 15 code points. -0-0-6-0-0.4-2
4-Oh good point. Maybe the person who wrote the blog post made the same mistake I did and swapped code point for code unit?75% overhead really does seem wasteful, you're right. what would that be used for? the utf16 bridging indices?-0-0-2-0-0.3-3
5-&gt; Oh good point. Maybe the person who wrote the blog post made the same mistake I did and swapped code point for code unit?The author of the post seems to be the main technical author of the changes, I would expect their language to be exactingly precise here as they've probably been dreaming in those terms for the last few weeks. Code unit is a very specific term of art when dealing with encodings.-0-0-2-0-0.3-4
6-That feel when you can’t afford a Mac -0-0--2-0-0--1
7-But, surely, you can afford Linux and slap Swift on it. -0-0-8-0-0-6
8-Good point You win-0-0-1-0-0-7
9-It sounds funny to be testing an HD video streaming site on a 3G connection. I suppose that's for signups rather than playing videos.They say they trimmed 200K from initial load by deferring the download of React on the landing page. Assuming that's gzipped and React+DOM account for 45K I wonder what the rest of it was. Redux, ReactRedux and Router would be pretty small.-0-0-22-0-0.4--1
10-Replaced React with Vanilla JS.It's not a big surprise however. Removing React speeds up pages.-0-0-9-0-0.3-9
11-&gt; Since most of the elements on the page were basic HTML, remaining elements such as JavaScript click handling and class adding could be replaced with plain JavaScript, and the page’s language switcher, originally built using React, was rebuilt in vanilla JavaScript using less than 300 lines of code.&gt; &gt; The complete list of components ported to vanilla JavaScript were:&gt; &gt;  Basic interactions (tabs halfway down the homepage)&gt; &gt;  Language switcher&gt; &gt;  Cookie banner (for nonUS visitors)&gt; &gt;  Clientside logging for analytics&gt; &gt;  Performance measurement and logging&gt; &gt;  Ad attribution pixel bootstrap code (which are sandboxed in an iFrame for security)-0-0-7-0-0.5-10
12-Yep. We do this that way too. Basically your primary APP your whole page is about (or specific view) is react app. Everything else  either HTML/CSS or vanilla JS.This way website is really responsive even on low tier phones.I remember when one guy introduced react in company I worked with. I'm sitting there, looking at him and... does lang switcher that have literally 2 links must be a full react app? With over 0.5MB vendor library?-0-0-2-0-0.4-11
13-&gt; Removing React speeds up pages.Downloading less downloads faster? Sure. I doubt that using React slows down app performance unless you're doing something really wrong.-0-0--6-0-0.2-10
14-... What?-0-0-5-0-0-13
15-"Removing react speeds up pages" is a senseless statement. If you remove it, the page would break. If you rewrite your React app with vanilla JS, then unless you're careful to write very specific, optimal implementations for things like lists with dynamic elements or lots of complicated layout changes due to async state changes, you could easily end up with something slower.-0-0-3-0-0.3-14
16-Makes sense to me  I remember the first time someone told me about Netflix I looked up their website on my phone. That's the kind of pages they seemed to be optimizing here.-0-0-3-0-0.1-9
17-The Motorola 6502 had a very simple Instruction Set Architecture (ISA). It wasn't uncommon for 14 year olds to write games with it. The Beeb for example came with a small booklet which covered it pretty thoroughly.  I don't think this was it (its been 10+ years since I've seen it) but its closest I could find at an instant. [http://www.primrosebank.net/computers/bbc/documents/2339\_001.pdf](http://www.primrosebank.net/computers/bbc/documents/2339_001.pdf)-0-0-5-0-0.3-16
18-&gt; Motorola^*[MOS](https://en.wikipedia.org/wiki/MOS_Technology)-0-0-9-0-0.1-17
19-there were a fair few old spectrum/bbc/amstrad etc games that were written by teenagers weren't there? 90s me was in awe, because by then you'd need _at least_ 3 of you to make a good game and it would take you like _a month_-0-0-2-0-0.2-17
20-&gt; you'd need *at least* 3 of you to make a good game[...](https://youtu.be/HrNiGYdvyQ4?t=566)-0-0-1-0-0.2-19
21-&gt;(big error I used to make when typing in the Assembler code: not saving the program before running it, because unlike Basic games, you could write some code that crashed the computer and then wrote over the code you'd just typed in)Oh, yes, that brings back some memories... The dreaded "Bad program" message and the slow realisation that you've lost a evening's worth of onefinger typing...-0-0-1-0-0.2-16
22-^The linked tweet was tweeted by [@KevEdwardsRetro](https://twitter.com/KevEdwardsRetro) on Nov 05, 2018 21:46:14 UTC (72 Retweets | 176 Favorites)I have just released the 6502 source code for my BBC Micro game, Crazee Rider, from 1987. This was taken from the original master disks and adapted to build using BeebAsm on a PC. I hope this gives some insight into how games were made in the 1980s![https://github.com/KevEdwards/CrazeeRiderBBC](https://github.com/KevEdwards/CrazeeRiderBBC) [Attached photo](https://pbs.twimg.com/media/DrRQ6dvW4AEe49X.jpg:orig) | [imgur Mirror](https://i.imgur.com/QZxfoAd.jpg)^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •-0-0-7-0-0--1
23-I have happy memories of playing this on my Acorn Electron back in the 80s. Just seeing that screenshot put a smile on my face. :)-0-0-3-0-0.1--1
24-wow, didn't realize 6502 was still in use in 1987!-0-0-2-0-0.1--1
25-6502 is still in production (literally millions of them a year are used in embedded space).  It will still be around in a century, driving some widget people’s grandads rely on. -0-0-3-0-0.2-24
26-It'll still be around in 10 centuries, as seen [here](http://pixel.nymag.com/imgs/slideshows/2011/06/futurama/signatures/20110620_futureramabenderxraysigned.nocrop.w670.h375.jpg).-0-0-1-0-0.1-25
27-6502 was just about the perfect platform to learn to code in assembler.  Simple memory models, color lookup tables, easy to understand  interrupts. I so clearly remember that moment when I finally "got it" about how to do smooth graphics by coding screen buffer refreshes during the vertical blanking interval.It was pretty easy to code straightforward algorithms in 6502. But there was plenty of room for the kind of cleverness and indirection, macros and selfmodifying code necessary to overcome the tiny tiny memory footprints of the day.  A great foundation  that prepared me for coding in C a few years later. &amp;#x200B;Thank you, Motorola, for the 6502.  -0-0-1-0-0.3--1
28-you mean ```#define private public``` is bad practice ? oh..-0-0-123-0-0.2--1
29-Holy shit. Is this used in the real word? Does this Works? If I go to a coding interview where I got asked "how access private members using only standard C++, without any external tool" I don't think I would came up with something like that.-0-0-22-0-0.3-28
30-Yes, this works, because the preprocessor is a separate step that happens before the C++ compiler sees any files. The preprocessor has no knowledge of C++ keywords, it just does text replacement.-0-0-20-0-0.4-29
31-The standard says no it doesn't work, but in practice compilers don't really check for this so it's up to you to follow the rule.-0-0-14-0-0.3-30
32-Which part of the standard? It's worked with every compiler I've ever tried it on-0-0-2-0-0.1-31
33-From the standard itself:17.4.3.1.1 Macro names [lib.macro.names]&gt; 1 Each name defined as a macro in a header is reserved to the implementation for any use if the translation unit includes the header.164)&gt; 2 A translation unit that includes a header shall not contain any macros that define names declared or defined in that header. **Nor shall such a translation unit define macros for names lexically identical to keywords.**Taken from [this answer](https://stackoverflow.com/a/9109427/2104697).But indeed it still works on every compilers because they won't check for that, and people still use it so enforcing it would break unit test code.Although in my opinion, if you're doing that you are looking for trouble.-0-0-22-0-0.5-32
34-I stand corrected. I did not know about that rule in the standard.-0-0-3-0-0-33
35-I’ve mostly seen it used for testing purposes. If this is in production code, then whoever made that code needs to reconsider their life decisions.-0-0-3-0-0-29
36-If I had a dollar for every time I could quote this...-0-0-3-0-0-35
37-The standard says it's undefined behavior to redefine keywords. The compiler is free to replace your whole program by a unicorn if you do this.-0-0-5-0-0-29
38-&gt; Is this used in the real word? Does this Works?Yes and yes, I've seen it happen.-0-0-2-0-0-29
39-ON ERROR RESUME NEXT-0-0-1-0-0-29
40-Google Test uses it. -0-0-1-0-0-29
41-In a world where the `friend` keyword doesn’t exist…-0-0-21-0-0--1
42-Yep, where I work, for a couple classes we just forward declare the test fixture's name in the tobetested class and friend it. No major harm done.-0-0-8-0-0.2-41
43-This-0-0-2-0-0-41
44-He says he's doing this to expose private variables for testing, but... I thought the whole point of private variables is that they should never be tested outside the class?  The whole point of *having* private fields is hiding them from the outside world, so that you can change them.  You're deliberately excluding them from the contract you have with users, so I'm having trouble imagining where testing them from outside the class would be useful.  The only methods and variables you should be testing from an external framework, as far as I know, are those that have been marked public.   Private stuff should be tested internally, or not at all.-0-0-96-0-0.3--1
45-&gt; You're deliberately excluding them from the contract you have with users, so I'm having trouble imagining where testing them from outside the class would be useful. imagine that you're implementing a large algorithm composed of 6 subalgorithms in a research paper.  In addition you add a few other subdecompositions because it makes sense and makes the main algorithm code cleaner and easier to read &amp; follow. You don't want the subalgorithms to be in your public API since the only reason for them to be called is as part of the main algorithm  some steps that occur multiple time for instance, but you really want to test them independently because running the main algorithm takes 10 minute on trivial cases and you don't want to wait 10 hours every time you change something because the test suite only calls the main algo while you changed a substep which has its own contracts. What can you do ?  -0-0-38-0-0.5-44
46-You can provide the support methods in a separate module, marked as a "support" module. This would make clear that, while the module may still be accessed, it doesn't provide meaningful features and is not part of the public API. Maybe you're also able to provide the module as packageprivate so that when the library is compiled the symbols of the module are not exported.This also provides cleanliness and separation of concern, allows you to consider edge cases aside from the problem you're solving and if the support module is generalpurpose enough it may even become a library by itself.-0-0-48-0-0.4-45
47-&gt; This would make clear that, while the module may still be accessed, it doesn't provide meaningful features and is not part of the public API. hahaha... yeah, no, from my experience this never works in practice. everything in a `detail` namespace will start to be depended upon by someone.&gt; when the library is compiled the symbols of the module are not exported.maybe it has to be shipped as a template or inline library for performance concerns-0-0-0-0-0.3-46
48-&gt;it doesn't provide meaningful featuresYeah, maybe I didn't explain my idea the right way, I'm sorry. What I meant is that, if I started using a computer vision library, I wouldn't usually be interested in using the \`org::something::cs::research::computervision::algorithm::matrix::utilities::computation::eigenvalue()\` function, unless it's strictly necessary. And if I had to, why I shouldn't be allowed to? It has enough logic (even if lowlevel) to be worth unittesting.I think depending on utility packages is not bad by itself, as long as you're not promising anything spectacular in your signature.    double eigenvalue(const double[][] matrix)is a good utility function and I wouldn't mind if someone took my code 6 months from now and wrote other APIs using that function. In fact I'd be honored :) but this may be starting to be off topic...In the end however, the principles of TDD are roughly the same: if it ain't public it ain't worth testing (aka don't write private members).-0-0-14-0-0.5-47
49-&gt; everything in a detail namespace will start to be depended upon by someone.Don't you do code reviews?  If you can't prevent developers from doing bad things, perhaps you should get better developers or a better system.-0-0-6-0-0.2-47
50-&gt; If you can't prevent developers from doing bad thingswell, no you can't because your code is out there on the open and the day you change any observable behaviour you get flooded with github issues of people who depended on it-0-0-11-0-0.1-49
51-What can you do? You friend the test class and move on.-0-0-3-0-0.1-45
52-I know friends are dangerous, but Idk why more people don't suggest it wrt testing libraries. It's one of the 2 narrow exceptions where the  convenience is worth the alternative's levels of indirect ion w/o sacrificing all your integrity. -0-0-1-0-0.1-51
53-I don't see why using friend classes in testing would be dangerous. It is a targeted access for a specific class/function, not for everyone. It is a common practice and very effective.Only downside being that you have to actually write two lines of code in the target class.-0-0-1-0-0.1-52
54-&gt;imagine that you're implementing a large algorithm composed of 6 subalgorithms in a research paper.  In addition you add a few other subdecompositions because it makes sense and makes the main algorithm code cleaner and easier to read &amp; follow. You don't want the subalgorithms to be in your public API since the only reason for them to be called is as part of the main algorithm  some steps that occur multiple time for instance, but you really want to test them independently because running the main algorithm takes 10 minute on trivial cases and you don't want to wait 10 hours every time you change something because the test suite only calls the main algo while you changed a substep which has its own contracts. What can you do ?I'd use `private package`\construct in Ada. \[[Link](http://archive.adaic.com/docs/reports/cohen/child/intrchld.txt)\]-0-0-1-0-0.4-45
55-It's not a perfect solution, however you can put lots of asserts in the main and sub algorithms. If you have taken the algorithms from a research papers they should have a correctness proof so it should be even easier. If there is something wrong the test should stop at the beginning without checking the final result.I used this technique while implementing a model checker for a temporal logic and it was quite useful.-0-0-1-0-0.3-45
56-Great question, in C# the answer is very easy.Dependency Injection + Mocking.I have a great blog post here that goes into details on why Unit Testing with Dependency Injection and Mocking perfectly solves this problem.http://blog.technically.fun/2018/06/inheritancevscomposition.htmlBasically speaking, you should follow proper Seperation of Concerns and break those algorithms up into their own classes, properly interface them, and make each class only depend on the interfaces of what it uses.Then, when you unit test, simply inject in 'mock' interfaced classes that just dummy the methods.-0-0-1-0-0.5-45
57-You try to describe one subset of numerous other possibilities.It is evident that OOP can not model all things equally well.A car, as class/object, is not the same as some abstract idea/algorithm.-0-0--1-0-0.2-45
58-&gt; What can you do ?Several things.You can `#define private public`, but only for internal testing (better done via a compiler flag).Better yet, you can develop the subalgorithms with tests for them, and after they're done, you make the subalgorithms private and delete all private tests. From then on, if the public interface tests aren't sufficient, they're bad and whoever wrote them should feel bad.If an algorithm is taking 10 hours to test... I don't know what's going on.-0-0--3-0-0.3-45
59-The real question is why are these methods private?Simply speaking, having private methods that require individual unit testing is a code smell. Why did they get declared private anyways?Instead, you need to take a step back and look at your architecture, because Id bet 50 bucks if I looked at your code and reviewed it, I would be able to refactor the key parts of the algorithms you are testing into decoupled public methods, which then can be unit tested.-0-0--2-0-0.4-58
60-&gt; I would be able to refactor the key parts of the algorithms you are testing into decoupled public methods, which then can be unit tested.what's the point of having a public method if it is a substep of an algorithm that no one except this algorithm is expected to call ? public APIs should be as small as possible.-0-0-12-0-0.3-59
61-Lets word it the other way round.Whats the pointing of hidding methods then wasting a bunch of time making akward roundabout unit tests just to save a couple TPUs on your api?There may be one, if you are, say, managing an API on tge scale of google or amazon, where your pipe handles billions of requests and every microoptimization counts.If not though, you are probably burning company hours on a micro optimization no one cares about.-0-0-0-0-0.3-60
62-&gt; wasting a bunch of time making akward roundabout unit tests just to save a couple TPUs on your api?    #define private publicThat doesn't "waste a bunch of time"-0-0-1-0-0.2-61
63-I'm struggling to interpret this any other way besides:* don't have private methods* don't unit test private methods. If there's something else you meant, please clarify. &gt; Why did they get declared private anyways?Go back to the basics. You make an interface private so users can't call it directly. So you could factor out a single function that does one thing. In the case of a word processor, a function that detects one misspelled word. Or builds a dictionary. That doesn't need to be public. Why go out of your way to eliminate private? When redefining it for testing works fine?-0-0-1-0-0.4-59
64-What you say is true, but not always possible. As mentioned in motivation section, code shall be structured in the way that it's testable. So leave what is private as private.   Unfortunately that is not always that easy. It's common that architecture is in some sens bad and maybe that project has big chunk of legacy code. Most probably because of that, some part of code will not be reachable for testing. If architect/developers realize that they have two options:1#  A) Let's do it in the proper way, and refactor this piece of crap  B) "Don't have time" / "there is deadline" / "it works for now" etc., and it's left .... for ages maybe. In most cases management makes that decision&amp;#x200B;This state is waiting there for some time, until client or management decides that we got to have tests. Most probably they will have some numbers for coverage, like 100% / 80% (function/branch) coverage. Then they go back to decision 1#, and if you are lucky they pick A, but in many many many cases management will say that they realize the problem but simple there is no time or even worse they don't give a crap and would not let to spend x amount of time just to make code reachable for tests.   But the tests got to be written. In this case you have a lot of options but u are breaking the rules anyhow.  This might sound as a worst case situation which does rarely happen and if it does it only happens in small companies with junior developers. But you'd be surprised.  And to even extend, imagine that you are a contractor and in fact your vote is not that important despite the fact that you have been warning them about the consequences.   But in perfect world where one can create architecture in the way it should be done, have tests from the very beginning, and can correct the mistakes in proper way even if it means refactor, then yes I fully agree that doing this kind of private data members access is just simply stupid and wrong.  -0-0-8-0-0.7-44
65-The main usecase for this (like they explicitly mention in the Readme) is for Legacy Code. There's some horrible working code that's been inherited, code that has been accidentally designed to be untestable, so short of changing all private variables and functions to public, there's this framework. The ideal solution is to refactor or rewrite the code, but it's not always possible or desirable considering time and resource constraints. Best thing you can do in these cases is to surround the legacy code with unit tests (which might mean testing private functions) and slowly rewrite parts of it as new features are requested/bugs are fixed.-0-0-2-0-0.3-44
66-Pretty much this. Private stuff is tested implicitly via the public contract.-0-0-4-0-0.1-44
67-As with everything, design patterns/principles are generalized guidelines, not strict rules. Yes, for the vast majority of classes you should not need to expose private properties and unit tests should be treated just as equally as outside consumers of the class/interface. However, there are rare exceptions to this. This is personally why I like the internal modifier in C#, because it lets me expose this information in an explicit manner that's still "private" to the anyone consuming the API outside the project.-0-0-5-0-0.3-66
68-Only in the C++ OOP model.-0-0--10-0-0.1-66
69-&gt; Private stuff should be tested internallyYeah, this project is intended to be just another approach at doing that, I think. Generally if you make a class `FooProvider`, there's nothing inherently wrong with also having tests for it that test its internal methods rather than just its interface  just like when you have, lets say, some sort of microservice that performs a relatively complex task, you wouldn't just have tests for its API.Google Test and other C++ testing frameworks do allow you already to do this, but to do it they require you to 'befriend' the class, meaning `FooProvider` class will have to declare a 'friendshipdependency' on your `FooProviderTester` class.So this is mainly a way to do the same thing you would already be able to do with other testing frameworks like Google Test by putting `FRIEND_TEST(FooProviderTester, ...)` into your `FooProvider` class, except without having to add code/dependencies/includes to it.-0-0-3-0-0.3-44
70-The problem if you test internal methods rather than the interface is that it leads to shakey, fragile unit tests. If you do proper OOP, the interface should be the most stable part of the class, it should change rarely, so the unit tests performed on it should also remain very stable. The implementation behind it (private methods and variables) are the most likely to change, so if you test the private methods, you are going to have to change your unit tests every time you change the implementation details, meaning you spend more time fixing unit tests than writing useful tests. Very bad cases of fragile unit tests mean that you can't rely on your unit tests to figure out when behaviour has been broken because so many are constantly breaking for unknown reasons.An unmaintainable test suite is a very real problem.-0-0-2-0-0.4-69
71-I do agree that this is how it should generally work, but I also think there are occasionally legitimate circumstances where you'd want to test private methods, so I wouldn't be too dogmatic about it personally.-0-0-2-0-0.2-70
72-If you're testing private methods, it's a sign of bad architecture. Anytime unit testing is hard/bothersome, it's a sign that the architecture is bad. The only reason you should test private methods is in a bad architecture (like Legacy code) where you do not have the time or resources to rewrite or refactor the old code. Otherwise, you should fix your architecture, which will make your code more testable.-0-0-0-0-0.3-71
73-When I read things like that I am happy that ruby doesn't follow the C++ OOP model (at the least nowhere near as a close 1:1).And I like to quote Alan Kay here (actually too many quotes to pick just one), be it in regards to the OOP model of Java or C++.-0-0-2-0-0.2-44
74-A good example of when you _may_ want to access internals is when you override streams, but then you have to crate friends -0-0-1-0-0.2-44
75-One very normal use case is to "initialize" the class being tested by the test case exactly the way the test case wants. This can  however be easily achieved by using friend classes/functions. No need for extra cheats and performance killing tricks.Friend class is a very clean, simple, targeted and safe way to provide access. Of course you shouldn't go around a shitty sw design by using friends in every situation but in testing it is a standard procedure.-0-0-1-0-0.3-44
76-I don't think it's unreasonable to run an operation the use your unit test assertions to test the values of internal state.That doesn't mean you ever want them exposed anywhere else.However, personally for that purpose I run my tests in a unit test class and make that class a friend of the class under test. That seems far better than making the test functions members of the tested class."Remember in c++ your friends can see your privates."-0-0-0-0-0.3-44
77-Goddamnit, C++.-0-0-11-0-0--1
78-Evergreen comment.-0-0-5-0-0-77
79-The fact that someone *wants* to do this indicates a lack of understanding as to what the original intention was.-0-0-9-0-0.1--1
80-C++ isn't some sacred, golden cow of design, nor is the private keyword. While in general, you'll want to not access private members to prevent unnecessary dependence on them, if you as a third party really want to test some particular aspect of a class's implementation, full well acknowledging the possibility of breaking change, there's no fundamental reason to stop this.-0-0-13-0-0.3-79
81-Its not a sacred cow, but if you look at the stated objective of the author, there already exists a number of builtin facility for doing exactly what he claims to want (the friend keyword being the most obvious).  He flat out says its better than "friend" without *any* justification.  Even in a legacy code situation, you have the actual header files, and you're compiling the damn thing, so its incredibly unclear why its "better" to do this huge hack than use a builtin keyword.-0-0-1-0-0.2-80
82-Implementing tests seems like such a pain in C++. Fortunately modern languages recognize that private functions still need to be tested.-0-0-1-0-0.2-80
83-This is a repost from [cpp subreddit](https://www.reddit.com/r/cpp), [original post](https://www.reddit.com/r/cpp/comments/9sdaw1/my_library_to_legally_access_private_data_members/)-0-0-1-0-0--1
84-This is a crosspost. It would've been a repost if you'd posted it again to the cpp subreddit.-0-0-7-0-0-83
85-Just in case someone was wondering, this has a severe runtime impact. I ran a very simple loop comparison:```int main() {   Foo f;   for (int i = 0; i &lt; 500000; i++) {      f.getXRef()++;      // ::accessor::accessMember&lt;FooX&gt;(f).get()++;   }   std::cout &lt;&lt; f.getXRef() &lt;&lt; std::endl;   // std::cout &lt;&lt; ::accessor::accessMember&lt;FooX&gt;(f).get() &lt;&lt; std::endl;   return 0;}```&amp;#x200B;Compared to just calling a simple member nonconst reference getter (which is not a great idea, but I wanted to be at least comparable), this code jumped from running in 4ms to 21ms on average with the accessMember templates. I'm guessing we have to instantiate a std::ref, and maybe even some other objects transiently, but I don't really understand his template magic enough to fully explain what is the true cause.&amp;#x200B;So this may be "ok" in tests, but a very severe warning to using this in production, beyond the obvious smelliness.-0-0-1-0-0.5--1
86-Well, there goes like 90% of the porn sites.-0-0-8-0-0.2--1
87-Dear Webmaster,Do you support your business with ads? It would be a shame if something were to ... happen ... to them. Don't risk your source of revenue being blocked. Sign up for Google Ads to ensure that ads on your website will always be displayed! Don't take a chance with those ... ehem ... _other_ networks.\ ~~The Mafia~~ Google AdSense-0-0-6-0-0.3--1
88-Chrome to block google competitor's ads. Betcha abusive ads from Google ad services gets a free pass.-0-0-7-0-0.2--1
89-We can assure you that any "abusive" ads on our network are an honest mistake!-0-0-1-0-0.1-88
90-That is a terribly misleading title. -0-0-2-0-0.1--1
91-Not terribly misleading. But yeah, before reading the article, I though it meant that Chrome would block the entire abusive site.But then you see it doesn't say "block" but "adblock." So if the site doesn't comply, then Chrome will block all the ads on the site.I'm okay with that too.-0-0-2-0-0.3-90
92-&gt; I'm okay with that too.That is dangerous territory for a company running a competing ad network. I am definitely not "okay" with that.-0-0-1-0-0.2-91
93-This isn't good news, it's textbook monopolistic behavior. Google has much of the browser market share, so them choosing which ads are kosher means they can obtain a tighter monopoly with their own ad network.-0-0-2-0-0.2--1
94-I'm getting tired of seeing articles like this.  "Breakthrough performance in &lt;difficult task&gt; is nothing more than &lt;new solution to difficult task&gt;".  Yes, that's what literally all research and progress is.  You could write the same argument about the industrial revolution: "The Watt Steam Engine is simply expensive hardware and PR thrown at a common cookfire" and be no less accurate and no less misleading than this article.-0-0-62-0-0.4--1
95-I don't think it's similar to that at all. When the steam engine came out, it was just viewed as a better engineering solution to an existent problem. Nobody has any problems with ML/AI being framed in that way; it's just undeniable that incredible progress has been made in performance in tasks like image recognition, spam detection, game playing, etc, in the last 20 years.The problem is when influential people who should know better get mystical about their creations, to the point of claiming real human attributes to something that the end of the day is just a much fancier version of a linear regression. I think this quote nails it on the head:&gt;With such a torrent of exaggerations and anthropomorphisms being used to  describe what are, essentially, dumb and mechanistic systems&amp;#x200B;People working on ML are doing very valuable work, but understanding intelligence, creativity, intuition, is not part of that work because their creations don't display it. What's doubly and triply and insulting is that nowadays in the minds of people on proggit, programmers, and even much of the media and general population, AI/ML researchers are some of the main contributors to understanding these things. In reality, they are not. There are tons of very smart people working on understanding these things, and you don't do it by building a program that can do a very narrow task (like Go) better than a human and then making a few trite headlinegaining statements to the media along the way. You do it by working very hard building models and collecting empirical data on systems that are known to display real intelligence and openended problem solving ability, i.e. animals and humans. I did some work in neuroscience and there are some truly amazing people doing amazing things; on their behalf whenever I see these overblown statements from AI/ML researchers I roll my eyes.&gt;I'm getting tired of seeing articles like this.... Yes, that's what literally all research and progress is.And yet, you literally almost never see articles of this nature about any other field. It's pretty unlikely that a bunch of people just happen to have a vendetta against ML. If tons of progress is being made in all kinds of different fields, and these articles don't get written, you have to wonder why, and the answer is quite simple: in most other fields the overselling and overhyping is nowhere near as dramatic, so you don't get that backlash. Pretty much the whole article is about that; "hyperbole" is in the first sentence. I do think the article goes a bit too far at points ("In other words, there has not been any significant [conceptual progress](http://www.technologyreview.com/s/608911/isairidingaonetrickpony/)"), but overall I agree with it. If you're tired of articles like this, and I'm tired of AI researchers ridiculous claims of "creativity" and "discovery" and "genius", let's put an end to the latter and I'm confident that the former will end shortly thereafter.-0-0-42-0-0.6-94
96-*You* are, essentially, 'a dumb and mechanistic system'. The fact that we don't understand the underlying mechanics to an exacting detail or the entire construction in no way changes this.This is the problem with AI research and has been for years. The moment you understand how it works, the magic 'intelligence' goes away. Are their limitations in the system and is it inherently limited compared to human (or even animal!) intellect? of course. But we don't need that to achieve the results we need for current products. Will we get there in time? sure.here. *define* creative. Is it the ability to create novel solutions to current problems, often based on the current or other context? well shit, these machines sure sound 'creative' to me. Part of the problem is that people see 'creative' and assign magic to it and don't see it for what it is. a few neurons in your brain connected in just the right way and start firing more strongly then it was before, all based on connections formed over the current problem. Do I think it's more complex then that? of course it is. but it's not magic either.-0-0--9-0-0.4-95
97-&gt;You are, essentially, 'a dumb and mechanistic system'. It hasn't been proven that we're mechanistic at all, actually.  That's a very famous big open question. &gt;Will we get there in time? sure.Also a big open question.  We don't *technically* know that computers are capable of that yet.  In summary, you're stating things as known facts that that are at best likely to be true but very unsolved. -0-0-5-0-0.3-96
98-&gt; It hasn't been proven that we're mechanistic at all, actually. That's a very famous big open question.According to everything we know and literally all the evidence. Since we don't *actually* know how it entirely works, that doesn't mean it couldn't be something else, but come on now...&gt; Also a big open question. We don't technically know that computers are capable of that yet.Everything we know literally says that thinking is a process that doesn't even require quantum mechanical processes. It's mostly electrochemical. Computers can simulate that. Further, everything we know so far suggests this is electrochemical *because it's meat*, not because it has to be, meaning we can simulate a *simplified* version.Just because we don't know something completely and to the nth degree, does not mean we can't make reasoned and reasonable conclusions. Your brain is a computing meat, it uses electrochemical systems to do it. It's messy and complicated, but it's *just* messy and complicated. It's not magic and it's not purely limited to a singular structural form that can't be reproduced in any other way. (even then, we could build purely meat 'computers')-0-0--1-0-0.5-97
99-&gt; Since we don't actually know how it entirely works, that doesn't mean it couldn't be something else, but come on now...We know very little about how it works, actually. &gt;Everything we know literally says that thinking is a process that doesn't even require quantum mechanical processes. We don't know that at all, actually.  You're full of yourself. -0-0-4-0-0.3-98
100-Youve made the same mistake he rails against &gt; will we get there in time? sureTheres no way real intelligence is going to arrive miraculously out of these techniques. Or anything truely 'creative'.-0-0-2-0-0.2-96
101-&gt; Theres no way real intelligence is going to arrive miraculously out of these techniques.and he uses a super fuzzy definition of intelligence and ignores that the techniques being used are no longer the techniques of before. They used to be *literally* just extensive linear algebra. Value goes through, is transformed by a pure function (in the mathematics sense) and out comes an answer. We still use this technique where appropriate, but we now have feedback loops and memory etc etc etc. It's nonlinear systems feedbacking into nonlinear systems. We have left those techniques *far* in the dust.I've just shown that we all *ready* have creativity in these systems. Even many of the simple systems have creativity (they being a function of the training behavior to produce the creative solutions, rather than the solution, ie the network, being creative themselves). The creation of novel solutions to problems. that is creativity. Anything more then that you will have to give a more formal definition.-0-0--2-0-0.4-100
102-The mathematics you're talking about were developed decades ago.  Neural networks were first proposed in the 60s, and deep learning was first proposed in the 80s.The biggest advances spurring the recent AI revolution have nothing to do with nonlinear systems feeding back into nonlinear systems.  It's because we have datasets large enough to be useful, computers powerful enough to run existing algorithms in a reasonable amount of time, and software architectures robust enough to handle very large teams of engineers working out the fine details of the problems.-0-0-2-0-0.4-101
103-But then by this attitude there's no such thing as AI. Really, everything is just an improvement over something previous, even actual human intelligence. At what point could we define an algorithm as creative or intuitive? Using adjectives like that to describe algorithms isn't inappropriate at all  to say it is is arrogant and implies that there's something unique and special about the human cognition you limit those words to. How close to human does an algorithm have to get before it counts? -0-0--2-0-0.3-95
104-You might get tired, but "literally all" is wrong.Yes, a lot of progress is incremental accumulation and brute force, but certainly not all.What you have to distinguish is a breakthrough in *results* from a breakthrough in *methods*. AI finally crushed a few long standing problems  such as image and human language recognition. But these are the results of breakthroughs in unrelated areas chip density and SIMD scaling.AI, at heart, hasn't changed much. It's more like a breakthrough in horse production than a steam engine.^1A lovely analogy would be the history of ultramarine blue  once it's production was a closely guarded secret making it very valuable and rare. Technologically, it would have been possible to ramp up produciton significantly  with heavy investment in transport networks and massive manpower dedicated to it. The *breakthrough* was synthesis from common materials. ^1) ^(with apologies to every AI researcher who knows how much smart all the incremental improvements and bruteforcing have required.)-0-0-7-0-0.6-94
105-"Raytracing has been around since the 80s, Nvidia RTX is just a bunch of PR bullshit and expensive hardware"&amp;#x200B;Meanwhile, my company will be profiting off of hardwareaccelerated raytracing as it makes its way into mainstream use while a bunch of gamers rage at how hard it will be to convince their parents to buy them the latest and greatest graphics card for Christmas.-0-0-8-0-0.4-94
106-Hey, some of us are adults that rage about the latest graphics cards. Graphics cards don't grow on trees.-0-0-6-0-0.1-105
107-If GPUs don't grow on trees AI might be doomed. Maybe they should söndag AI research money on GPU trees?-0-0-1-0-0.1-106
108-Yeah, R&amp;D is expensive  the real reason behind the price-0-0--1-0-0.1-106
109-All the goddamn cryptocurrency miners are the real reason behind the price.-0-0-2-0-0.1-108
110-That bubble burst, crypto miners are on specialized hardware and no longer monopolizing the GPU market.-0-0-1-0-0.12-109
111-It's fun that we can have this argument about what is driving the cost of highspeed arithmetic processors in and out of our specific household budgets roughly a generation after PCs became affordable by mere mortals.Moore's law is basically Santa, if Santa were a thousand times more awesome and we took him totally for granted.-0-0-1-0-0.2-110
112-&gt;Meanwhile, my company will be profitingJust because you can profit off of expensive hardware and PR thrown at an old idea doesn't make it some kind of a conceptual breakthrough. You understand that commercial application/viability is distinct from fundamental advancement, right?-0-0-1-0-0.3-105
113-The watt steam engine was made in 1763; it was an improvement on other steam engine designs made in 1712.The first contemporary mention of an "industrial revolution" wasn't until 1800, a full 40 years later.Change just always going to be more gradual than people gave it credit for.-0-0-1-0-0.1-94
114-A selfdescribed innovator complaining that a thing they didn’t come up with has no innovation value sounds like sour grapes to me.“Why Google bought DeepMind while Thompson is relatively obscure” is a fake question. It should be obvious that Thompson’s project didn’t reach commercial success because it’s not reproducible: the model that you created on one FPGA doesn’t run on any other one. In my mind, the only reason this experiment is included in the story is that it hides a little bit the egocentric nature of the piece...-0-0-16-0-0.3--1
115-AI in the 90s was setting up advances we make today. look at [speech recognition](https://news.developer.nvidia.com/microsoftsvoicerecognitiontechnologyalmostasaccurateashumans/). Made possible by backprop and gradient descent methods, but also numerous other advances along the way. Backprop was never enough to train the deep models that we do today, and the author seems perfectly fine to reduce the progress that has been made to "backprop is the only conceptual advancement." Research is slow and incremental. The author is willing to admit that backprop took 30 years to gain recognition in neural nets, but now slow and incremental process for the next 30 years is not as interesting.I don't think it's worth addressing that AI isn't modeling the human brain or that they can't generalize to completely new situations. No one has claimed those things.-0-0-7-0-0.5--1
116-Yeah, I don't get the sour grapes.  I've worked alongside High Energy Physicists, and their entire field is some guy coming up with a theory that's completely impractical to test, then 30 years later they build some enormous (by previous standards) machine that makes it possible.  Nobody claims that the scientists involved in that machine aren't making huge advancements, even though the theory was 30 years old.Yes, a lot of the applications of AI and ML today are old news from a theory standpoint.  That doesn't make it any less amazing that we can not only actually implement them but make them actually useful products on mobile hardware.Plenty of ideas from the past remain too computationally intensive to be of use today, but they may fall too.-0-0-1-0-0.4-115
117-It's all in the headline: "mything the point". What I don't see (aside from parallel universe fantasies) are the growth of myths and mysticism around other fields that AI seems to attract. Perhaps it's just more easily misunderstood, and therefore easier to wax on about?-0-0-3-0-0.2-116
118-Every time they start up a new particle accelerator, there's a bunch of myths and mysticisms and hyperbole, such as the possibility of, say, destroying the universe by ripping a hole in reality.  High Energy Physics is a more rarefied discipline than programming, and so the exaggeration is more limited to people who have nothing close to understanding.Programming is populated by many, many levels of education and competence. The general consumer, likewise, has seen many magical things come to fruition in the last few years.-0-0-3-0-0.3-117
119-It's possibly just an echo of Jaron Lanier, who maintains that both VR ( his wheelhouse ) and AI are "myths". He doesn't say it in an uncharitable fashion and it can be seen to describe some of the more breathless verbiage that gets used on the subjects. -0-0-1-0-0.2-117
120-This article was actually a pretty great survey of the history of machine learning (and some of the practical problems).I do agree that there's a lot of hype, but that some of it is justified. "Creative" doesn't seem like a terrible way to describe something like [Deep Dream](https://deepdreamgenerator.com), and we have ML applications threatening to take away jobs like truck driver, investment analyst, legal clerk, ...I guess I think some of the hype is justified, just because of the applications (not to mention accessibility, because of TensorFlow), even if broad theoretical ground isn't being broken.Nonetheless, good article.-0-0-4-0-0.2--1
121-Headline is a bit off.  It, at least, also includes the pregeneration of massive collections of raw data made possible by the development things like social media and other internet datamining technology.-0-0-2-0-0.1--1
122-This is mostly true, but so what?  It's a commonplace by now that ideas are not worth much compared to careful engineering and execution and cost of implementation.  It's pretty incredible that anyone with a credit card can spin up a few supercomputers for a couple hours to train a neural network to recognize dogs and cats in pictures or whatever, it wasn't that many years ago that every single part of that was completely unfathomable.-0-0-1-0-0.3--1
